# ============================================================================
# FLUX.1-dev Identity LoRA - Fast Iteration Profile
# ============================================================================
# Source: DeepResearchReport.md Section 4.1
# Purpose: Validate dataset, detect early overfit, <1 hour runtime
#
# This is the SOURCE OF TRUTH for fast profile configuration.
# Loaded by scripts/build_train_cmd.py
# ============================================================================

[general]
# Paths (override via environment variables: MODEL_PATH, DATA_DIR, OUT_DIR)
pretrained_model_name_or_path = "/workspace/models/flux1-dev"
train_data_dir = "/workspace/lora_training/data/subject"
output_dir = "/workspace/lora_training/output"
output_name = "flux_lora_fast"
logging_dir = "/workspace/lora_training/logs"

[network]
# LoRA configuration
# P0 Fix: network_module is networks.lora_flux for FLUX (build_train_cmd.py enforces this)
# P0 Fix: network_alpha = network_dim (Section 4.1: rank/alpha 32/32)
network_module = "networks.lora_flux"
network_dim = 32
network_alpha = 32

[optimizer]
# AdamW8bit with fixed LR (alternative to Adafactor from report)
optimizer_type = "AdamW8bit"
learning_rate = 1e-4
text_encoder_lr = 0  # TE frozen by default

[scheduler]
# Constant with warmup (Section 4.1)
lr_scheduler = "constant_with_warmup"
lr_warmup_steps = 100

[training]
# Resolution and bucketing (Section 4.1)
resolution = 512
enable_bucket = true
min_bucket_reso = 256
max_bucket_reso = 768
bucket_reso_steps = 64

# Training steps
max_train_steps = 1500

# Batch and accumulation
# P1 Fix: gradient_accumulation_steps now wired
train_batch_size = 1
gradient_accumulation_steps = 4

# Precision (Section 3: bf16 + optional fp8_base via FP8_BASE=1 env var)
mixed_precision = "bf16"

# Memory optimization
gradient_checkpointing = true
blocks_to_swap = 18

# P0 Fix: noise_offset now wired (Section 4.1: 0.05 for fast)
noise_offset = 0.05

# Reproducibility
seed = 42

[saving]
save_every_n_steps = 500
save_model_as = "safetensors"
save_precision = "bf16"

[sampling]
# Sample generation during training
sample_every_n_steps = 250
sample_prompts = "/workspace/lora_training/configs/sample_prompts.txt"
sample_sampler = "euler"

[dataset]
# Caption handling
caption_extension = ".txt"
keep_tokens = 1  # Keep trigger token

[performance]
# Dataloader workers
max_data_loader_n_workers = 2
