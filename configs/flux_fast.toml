# ============================================================================
# FLUX.1-dev Identity LoRA - Fast Iteration Profile
# ============================================================================
# Source: DeepResearchReport.md Section 4.1
# Purpose: Validate dataset, detect early overfit, <1 hour runtime
# ============================================================================

[general]
# Paths (override via environment variables)
pretrained_model_name_or_path = "/workspace/models/flux1-dev"
train_data_dir = "/workspace/lora_training/data/subject"
output_dir = "/workspace/lora_training/output"
output_name = "flux_lora_fast"
logging_dir = "/workspace/lora_training/logs"

[network]
# LoRA configuration (Section 4.1: rank/alpha 32/32)
network_module = "networks.lora"
network_dim = 32
network_alpha = 32

[optimizer]
# Adafactor with fixed LR (Section 4.1)
optimizer_type = "Adafactor"
optimizer_args = ["relative_step=False", "scale_parameter=False"]
unet_lr = 1e-4
text_encoder_lr = 0  # TE frozen by default

[scheduler]
# Constant with warmup (Section 4.1)
lr_scheduler = "constant_with_warmup"
lr_warmup_steps = 100

[training]
# Resolution and bucketing (Section 4.1)
resolution = "512,512"
enable_bucket = true
min_bucket_reso = 256
max_bucket_reso = 768
bucket_reso_steps = 64

# Training steps
max_train_steps = 1500

# Batch and accumulation
train_batch_size = 1
gradient_accumulation_steps = 4

# Precision (Section 3: bf16 + fp8_base)
mixed_precision = "bf16"
full_bf16 = true
fp8_base = true

# Memory optimization
gradient_checkpointing = true
# block_lr = 0  # Enable if needed for VRAM

# Noise (Section 4.1: noise_offset 0.05)
noise_offset = 0.05

# Reproducibility
seed = 42

[saving]
save_every_n_steps = 500
save_model_as = "safetensors"
save_precision = "bf16"

[sampling]
# Sample generation during training
sample_every_n_steps = 250
sample_prompts = "/workspace/lora_training/configs/sample_prompts.txt"
sample_sampler = "euler"

[dataset]
# Caption handling
caption_extension = ".txt"
shuffle_caption = false
keep_tokens = 1  # Keep trigger token

[advanced]
# FLUX-specific
model_prediction_type = "raw"
# cache_latents = true
# cache_text_encoder_outputs = true
